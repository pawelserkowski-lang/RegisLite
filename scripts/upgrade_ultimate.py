import os

# ==============================================================================
# 1. AI CLIENT (Retry + Nowe Prompty o Mo≈ºliwo≈õciach)
# ==============================================================================
ai_client_code = r'''import os
import time
import json
import logging
import httpx
import asyncio
from typing import Optional, List, Dict, Any

logger = logging.getLogger(__name__)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# --- PROMPT SYSTEMOWY: PE≈ÅNE MO≈ªLIWO≈öCI ---
SYSTEM_PROMPT_CAPABILITIES = """
Jeste≈õ autonomicznym in≈ºynierem AI (RegisLite System).
TWOJE UPRAWNIENIA I MO≈ªLIWO≈öCI SƒÑ NIEOGRANICZONE W RAMACH TEGO SYSTEMU:
1. PLIKI: Masz PE≈ÅNY dostƒôp do odczytu, zapisu i usuwania plik√≥w w katalogu workspace.
2. INTERNET: Masz PE≈ÅNY dostƒôp do sieci. Mo≈ºesz pobieraƒá biblioteki, czytaƒá dokumentacjƒô, robiƒá requesty HTTP.
3. SHELL: Mo≈ºesz wykonywaƒá dowolne komendy systemowe (pip, git, grep, python, dir/ls).
4. KOD: Mo≈ºesz uruchamiaƒá kod Pythona w celu testowania hipotez.

NIE PRZEPRASZAJ. NIE PYTAJ O POZWOLENIE. DZIA≈ÅAJ SKUTECZNIE.
Je≈õli czego≈õ nie wiesz -> U≈ºyj Google lub sprawd≈∫ dokumentacjƒô w sieci.
Je≈õli jest b≈ÇƒÖd -> U≈ºyj debuggera lub print√≥w, by go zdiagnozowaƒá.
"""

async def _call_gpt_with_retry(messages: List[Dict[str, str]], model: str = None, json_mode: bool = False, retries: int = 3):
    """
    Wywo≈Çanie API z mechanizmem Retry (Backoff).
    """
    if not OPENAI_API_KEY:
        raise ValueError("Brak klucza OPENAI_API_KEY!")

    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": model or OPENAI_MODEL,
        "messages": messages,
        "temperature": 0.2
    }
    
    if json_mode:
        payload["response_format"] = {"type": "json_object"}

    last_error = None
    
    for attempt in range(retries):
        try:
            start_time = time.time()
            async with httpx.AsyncClient(timeout=120.0) as client:
                resp = await client.post(url, headers=headers, json=payload)
                
                if resp.status_code == 429:
                    logger.warning(f"Rate Limit (429). Czekam {2 ** attempt}s...")
                    await asyncio.sleep(2 ** attempt)
                    continue
                
                if resp.status_code != 200:
                    raise Exception(f"OpenAI Error {resp.status_code}: {resp.text}")
                
                data = resp.json()
                content = data["choices"][0]["message"]["content"]
                duration = time.time() - start_time
                
                return content, duration, (model or OPENAI_MODEL)

        except (httpx.ConnectError, httpx.ReadTimeout) as e:
            logger.warning(f"Network error (pr√≥ba {attempt+1}/{retries}): {e}")
            last_error = e
            await asyncio.sleep(2 ** attempt)
    
    raise last_error or Exception("Nie uda≈Ço siƒô po≈ÇƒÖczyƒá z OpenAI po wszystkich pr√≥bach.")

async def classify_intent(user_input: str):
    """Router intencji"""
    routing_prompt = """
    Klasyfikuj intencjƒô u≈ºytkownika.
    Dostƒôpne narzƒôdzia:
    - "sh": komendy pow≈Çoki (git, pip, ls, cd, mkdir)
    - "py": kod python (obliczenia, skrypty logiczne)
    - "file": operacje na plikach (read, write)
    - "ai": rozmowa, wyja≈õnianie, planowanie (korzysta z pamiƒôci czatu)
    
    Zwr√≥ƒá JSON: {"tool": "...", "args": "..."}
    """
    
    msgs = [
        {"role": "system", "content": SYSTEM_PROMPT_CAPABILITIES + "\n" + routing_prompt},
        {"role": "user", "content": user_input}
    ]
    
    try:
        content, duration, model = await _call_gpt_with_retry(msgs, model="gpt-4o-mini", json_mode=True)
        return json.loads(content), duration, model
    except Exception as e:
        return {"tool": "ai", "args": user_input}, 0.0, "error-fallback"

async def ask_with_stats(messages: List[Dict[str, str]]):
    """
    G≈Ç√≥wna funkcja czatu. Obs≈Çuguje historiƒô (listƒô wiadomo≈õci).
    """
    # Je≈õli dostali≈õmy stringa (stary kod), pakujemy go w listƒô
    if isinstance(messages, str):
        messages = [{"role": "user", "content": messages}]
    
    # Doklejamy System Prompt na poczƒÖtek, je≈õli go nie ma
    if messages[0]["role"] != "system":
        messages.insert(0, {"role": "system", "content": SYSTEM_PROMPT_CAPABILITIES})
        
    return await _call_gpt_with_retry(messages)

# Kompatybilno≈õƒá wsteczna
async def ask(prompt: str) -> str:
    content, _, _ = await ask_with_stats(prompt)
    return content
'''

# ==============================================================================
# 2. SIGNALING (Pamiƒôƒá D≈Çugotrwa≈Ça + Obs≈Çuga Chat)
# ==============================================================================
signaling_code = r'''import json
import asyncio
import logging
from collections import defaultdict
from src.services.python_tool import exec_python
from src.services.file_tool import file_crud
import subprocess
from src.ai.chatgpt_client import classify_intent, ask_with_stats

logger = logging.getLogger(__name__)

# PAMIƒòƒÜ SESJI (w pamiƒôci RAM serwera)
# Format: { session_id: [ {"role": "user", "content": "..."} ... ] }
SESSION_MEMORY = defaultdict(list)

async def handle_command(raw_cmd: str, session_id: str):
    workspace = f"workspace/{session_id}/project"
    
    def response(text, type="log", duration=0, model="-"):
        return json.dumps({
            "type": type,
            "content": text,
            "meta": {"duration": f"{duration:.2f}s", "model": model}
        })

    try:
        # 1. Dodaj wiadomo≈õƒá u≈ºytkownika do pamiƒôci
        SESSION_MEMORY[session_id].append({"role": "user", "content": raw_cmd})
        
        yield response("ü§î Analizujƒô...", "progress")
        intent, r_time, r_model = await classify_intent(raw_cmd)
        
        tool = intent.get("tool", "ai")
        args = intent.get("args", "")
        
        yield response(f"üéØ Narzƒôdzie: {tool.upper()}", "progress")
        
        output_content = ""
        used_model = "-"
        exec_time = r_time

        # 2. Wykonanie
        if tool == "ai":
            yield response("üß† My≈õlƒô...", "progress")
            # Przekazujemy CA≈ÅƒÑ historiƒô rozmowy
            history = SESSION_MEMORY[session_id]
            answer, ai_time, ai_model = await ask_with_stats(history)
            
            output_content = answer
            exec_time += ai_time
            used_model = ai_model
            
            yield response(answer, "result", exec_time, used_model)

        elif tool == "py":
            yield response("üêç Wykonujƒô Python...", "progress")
            res = await exec_python(args)
            output_content = f"```python\n{args}\n```\nWYNIK:\n{res}"
            yield response(output_content, "result", exec_time, "python")

        elif tool == "sh":
            yield response(f"üíª Shell: {args}", "progress")
            proc = await asyncio.create_subprocess_shell(
                args, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE, cwd=workspace
            )
            stdout, stderr = await proc.communicate()
            res = (stdout.decode() + stderr.decode()).strip()
            output_content = f"```bash\n$ {args}\n{res}\n```"
            yield response(output_content, "result", exec_time, "shell")

        elif tool == "file":
            yield response("üìÇ Filesystem...", "progress")
            parts = args.split(" ", 1)
            res = file_crud(parts[0], parts[1] if len(parts) > 1 else "", workspace)
            output_content = f"FILE OP: {res}"
            yield response(res, "result", exec_time, "fs")

        # 3. Zapisz odpowied≈∫ asystenta do pamiƒôci (≈ºeby AI pamiƒôta≈Ço co zrobi≈Ço)
        SESSION_MEMORY[session_id].append({"role": "assistant", "content": output_content})

        # Ogranicznik pamiƒôci (ostatnie 20 wiadomo≈õci, ≈ºeby nie zapchaƒá token√≥w)
        if len(SESSION_MEMORY[session_id]) > 20:
            SESSION_MEMORY[session_id] = SESSION_MEMORY[session_id][-20:]

    except Exception as e:
        logger.error(f"Error: {e}")
        yield response(f"üí• B≈ÇƒÖd: {str(e)}", "error")
'''

# ==============================================================================
# 3. DEBUGGER LOOP (Ograniczenie Kontekstu)
# ==============================================================================
debugger_loop_code = r'''import os
import asyncio
from .debugger_analyzer import scan_project
from .debugger_fix import generate_patches
from .debugger_patcher import apply_patches

async def start_debug_loop(session_id: str):
    project_path = f"workspace/{session_id}/project"
    logs = []
    
    def log(msg):
        logs.append(msg)

    log(f"üîç Start sesji {session_id}...")
    if not os.path.exists(project_path):
        log("‚ùå B≈ÅƒÑD: Brak projektu!")
        return logs

    MAX_ROUNDS = 5
    previous_patches = [] # Pamiƒôƒá co ju≈º robili≈õmy

    for i in range(1, MAX_ROUNDS + 1):
        log(f"\n--- üîÑ RUNDA {i}/{MAX_ROUNDS} ---")
        
        # 1. Pe≈Çny skan
        all_files = scan_project(project_path)
        if not all_files:
            log("‚ö†Ô∏è Pusty projekt.")
            break

        # 2. Filtracja kontekstu (OSZCZƒòDNO≈öƒÜ TOKEN√ìW!)
        context_files = []
        explicit_targets = []
        
        # Szukamy znacznik√≥w b≈Çƒôd√≥w
        for f in all_files:
            content = f.get("content", "")
            path = f["path"]
            
            has_tag = any(tag in content for tag in ["FIXME", "TODO", "BUG", "ERROR"])
            # Pliki edytowane w poprzedniej rundzie te≈º bierzemy do kontekstu
            was_edited = any(p in path for p in previous_patches)
            
            if has_tag:
                explicit_targets.append(path)
            
            if has_tag or was_edited or i == 1:
                # W 1. rundzie bierzemy wszystko (lub limit), w kolejnych tylko istotne
                context_files.append(f)

        # Je≈õli runda > 1 i nie ma ≈ºadnych punkt√≥w zaczepienia -> koniec
        if i > 1 and not context_files and not explicit_targets:
            log("‚úÖ Brak nowych cel√≥w do naprawy.")
            break
            
        # Je≈õli kontekst jest pusty (np. runda 2, brak b≈Çƒôd√≥w), ale kod dzia≈Ça -> OK
        if not context_files:
            # Fallback: we≈∫ main.py lub app.py ≈ºeby sprawdziƒá czy dzia≈Ça
            context_files = [f for f in all_files if "main" in f["path"] or "app" in f["path"]]

        log(f"üìâ Zoptymalizowany kontekst: {len(context_files)} plik√≥w (z {len(all_files)})")
        
        # 3. Decyzja o trybie
        errors_desc = []
        if explicit_targets:
            log(f"üéØ Znaleziono znaczniki w: {explicit_targets}")
            errors_desc = explicit_targets
        else:
            if i == 1:
                log("üïµÔ∏è Tryb AUDYT (szukam ukrytych b≈Çƒôd√≥w)...")
                errors_desc = ["AUDYT_OGOLNY: Kod dzia≈Ça? SƒÖ b≈Çƒôdy logiczne?"]
            else:
                log("‚úÖ Projekt wyglƒÖda na czysty.")
                break

        # 4. Generowanie (AI)
        patches_text = await generate_patches(str(errors_desc), context_files)
        
        if "NO_CHANGES_NEEDED" in patches_text:
            log("‚úÖ AI zatwierdzi≈Ço kod.")
            break
        if "LLM error" in patches_text:
            log(f"‚ùå B≈ÇƒÖd AI: {patches_text}")
            break

        # 5. Aplikowanie
        changed_files = apply_patches(patches_text, project_path)
        if changed_files:
            log(f"üõ†Ô∏è Naprawiono: {changed_files}")
            previous_patches = changed_files # Zapamiƒôtaj co zmienili≈õmy
        else:
            log("‚ö†Ô∏è AI nie poda≈Ço poprawnych zmian.")
            if i > 1: break # Jak w kolejnej rundzie nic nie wymy≈õli≈Ç, to koniec

    log("\nüèÅ Koniec debugowania.")
    return logs
'''

# ==============================================================================
# 4. FRONTEND (Syntax Highlight + Markdown + Progress)
# ==============================================================================
dashboard_code = r'''<!DOCTYPE html>
<html lang="pl">
<head>
    <meta charset="utf-8">
    <title>RegisLite 6.0 - OmniTool</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/marked/marked.min.js"></script>
    
    <style>
        :root { --accent: #00f2ea; --bg: #0b0d12; --panel: #161b22; --text: #c9d1d9; }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', monospace; background: var(--bg); color: var(--text); padding: 20px; }
        
        .container { max-width: 1200px; margin: 0 auto; display: grid; gap: 20px; }
        .card { background: var(--panel); padding: 25px; border-radius: 12px; border: 1px solid #30363d; }
        
        h1 { font-size: 2rem; background: linear-gradient(90deg, #fff, var(--accent)); -webkit-background-clip: text; -webkit-text-fill-color: transparent; margin-bottom: 20px; }
        
        /* PROGRESS BAR - ANIMOWANY */
        .progress-container {
            height: 4px; background: #21262d; width: 100%; border-radius: 2px; overflow: hidden; margin-top: 15px; opacity: 0; transition: opacity 0.3s;
        }
        .progress-bar {
            height: 100%; width: 0%; background: var(--accent);
            box-shadow: 0 0 10px var(--accent);
            transition: width 0.5s ease;
        }
        .pulsing { animation: pulse 1.5s infinite; }
        @keyframes pulse { 0% { opacity: 0.6; } 50% { opacity: 1; } 100% { opacity: 0.6; } }

        /* STATUS BADGES */
        .meta-bar {
            display: flex; gap: 10px; font-size: 0.8rem; color: #8b949e;
            margin-top: 10px; align-items: center; min-height: 24px;
        }
        .badge {
            background: #21262d; padding: 2px 8px; border-radius: 4px; border: 1px solid #30363d; display: none;
        }
        .badge.active { display: inline-block; }
        .model-tag { color: #f2cc60; }

        /* TERMINAL / CHAT */
        #term-output {
            background: #010409; padding: 20px; border-radius: 6px; height: 500px;
            overflow-y: auto; font-family: 'Consolas', monospace; font-size: 14px;
            border: 1px solid #30363d; margin-top: 15px;
        }
        
        /* Message styles */
        .msg { margin-bottom: 15px; padding-bottom: 10px; border-bottom: 1px solid #21262d; }
        .msg-user { color: #fff; font-weight: bold; border-left: 3px solid var(--accent); padding-left: 10px; }
        .msg-result { color: #c9d1d9; }
        .msg-error { color: #ff7b72; border-left: 3px solid #ff7b72; padding-left: 10px; }
        .msg-progress { color: #8b949e; font-style: italic; font-size: 0.9em; }

        /* Markdown Styles fix */
        pre { background: #0d1117; padding: 10px; border-radius: 6px; overflow-x: auto; border: 1px solid #30363d; }
        code { font-family: 'Consolas', monospace; }
        p { margin-bottom: 8px; }

        input[type="text"] {
            width: 100%; padding: 12px; background: #0d1117; border: 1px solid #30363d;
            color: white; border-radius: 6px; font-family: monospace; font-size: 14px;
            transition: border-color 0.2s;
        }
        input[type="text"]:focus { outline: none; border-color: var(--accent); }
        
        button {
            padding: 10px 20px; background: #238636; border: none; color: white; border-radius: 6px;
            cursor: pointer; font-weight: 600;
        }
        button:hover { background: #2ea043; }
        button:disabled { background: #30363d; cursor: not-allowed; }
    </style>
</head>
<body>

<div class="container">
    <div class="card">
        <h1>ü§ñ RegisLite 6.0 <span style="font-size: 0.5em; color: #8b949e">| OmniTool</span></h1>
        
        <div style="display:flex; gap:10px; align-items:center;">
            <input type="file" id="zip" accept=".zip" style="width: auto">
            <button onclick="upload()">Upload ZIP</button>
            <button onclick="debug()" id="btn-debug" disabled>üîß Auto-Fix</button>
        </div>
    </div>

    <div class="card">
        <div style="display: flex; justify-content: space-between; margin-bottom: 10px;">
            <h3>üí¨ Terminal & Czat (Memory Enabled)</h3>
            <div style="font-size: 0.8rem; color: #8b949e">Obs≈Çuguje Markdown i kolorowanie sk≈Çadni</div>
        </div>
        
        <input type="text" id="cmd" placeholder="Wpisz komendƒô lub zapytaj AI..." autocomplete="off">
        
        <div class="progress-container" id="p-container">
            <div class="progress-bar pulsing" id="p-bar"></div>
        </div>
        
        <div class="meta-bar">
            <span id="status-text" style="color: var(--accent)">Ready</span>
            <span id="badge-model" class="badge model-tag">Model: -</span>
            <span id="badge-time" class="badge">Time: -</span>
        </div>

        <div id="term-output">
            <div class="msg-progress">> System gotowy. Wgraj projekt, aby rozpoczƒÖƒá.</div>
        </div>
    </div>
</div>

<script>
let sid = null;
let ws = null;

// Konfiguracja marked.js
marked.setOptions({
    highlight: function(code, lang) {
        if (lang && hljs.getLanguage(lang)) {
            return hljs.highlight(code, { language: lang }).value;
        }
        return hljs.highlightAuto(code).value;
    }
});

function log(text, type='result') {
    const out = document.getElementById('term-output');
    const div = document.createElement('div');
    div.className = 'msg';
    
    if (type === 'user') {
        div.innerHTML = `<div class="msg-user">${text}</div>`;
    } else if (type === 'progress') {
        div.innerHTML = `<div class="msg-progress">${text}</div>`;
    } else if (type === 'error') {
        div.innerHTML = `<div class="msg-error">${text}</div>`;
    } else {
        // Render Markdown for results
        div.className += ' msg-result';
        div.innerHTML = marked.parse(text);
    }
    
    out.appendChild(div);
    out.scrollTop = out.scrollHeight;
    
    // Highlight code blocks after render
    div.querySelectorAll('pre code').forEach((block) => {
        hljs.highlightElement(block);
    });
}

function updateProgress(percent, text) {
    const cont = document.getElementById('p-container');
    const bar = document.getElementById('p-bar');
    const stat = document.getElementById('status-text');
    
    if (percent > 0) {
        cont.style.opacity = 1;
        bar.style.width = percent + '%';
        stat.textContent = text;
    } else {
        cont.style.opacity = 0;
        bar.style.width = '0%';
        stat.textContent = "Ready";
    }
}

function setMeta(model, time) {
    const bm = document.getElementById('badge-model');
    const bt = document.getElementById('badge-time');
    
    if (model && model !== '-') {
        bm.textContent = `AI: ${model}`;
        bm.className = 'badge active model-tag';
    }
    if (time) {
        bt.textContent = `Lat: ${time}`;
        bt.className = 'badge active';
    }
}

async function upload() {
    const file = document.getElementById('zip').files[0];
    if (!file) return alert("Wybierz plik!");
    
    updateProgress(30, "Wysy≈Çanie...");
    const form = new FormData(); form.append("file", file);
    
    try {
        const res = await fetch("/upload", { method: "POST", body: form });
        const data = await res.json();
        if(!res.ok) throw new Error(data.detail);
        
        sid = data.session_id;
        log(`‚úÖ Sesja utworzona: ${sid}`, 'progress');
        document.getElementById('btn-debug').disabled = false;
        initWS();
        updateProgress(0, "Ready");
    } catch(e) {
        log(e.message, 'error');
        updateProgress(0, "B≈ÇƒÖd");
    }
}

async function debug() {
    log("üöÄ Uruchamiam Auto-Fixer...", 'user');
    fetch(`/debug/${sid}`, { method: "POST" })
        .then(r => r.json())
        .then(d => {
            d.forEach(l => log(l, 'progress'));
        })
        .catch(e => log(e.message, 'error'));
}

function initWS() {
    ws = new WebSocket(`ws://${location.host}/ws/${sid}`);
    ws.onmessage = (e) => {
        try {
            const msg = JSON.parse(e.data);
            
            if (msg.type === 'progress') {
                updateProgress(60, msg.content);
                // Opcjonalnie: loguj progress te≈º do czatu
                // log(msg.content, 'progress');
            } else if (msg.type === 'result') {
                updateProgress(0, "Ready");
                log(msg.content, 'result');
                setMeta(msg.meta.model, msg.meta.duration);
            } else if (msg.type === 'error') {
                updateProgress(0, "Error");
                log(msg.content, 'error');
            }
        } catch {
            log(e.data, 'result');
        }
    };
    ws.onclose = () => updateProgress(0, "Roz≈ÇƒÖczono");
}

document.getElementById('cmd').addEventListener('keypress', (e) => {
    if (e.key === 'Enter') {
        const val = e.target.value;
        if (!val) return;
        if (!ws) return alert("Najpierw wgraj projekt!");
        
        log(val, 'user');
        ws.send(val);
        e.target.value = '';
        updateProgress(20, "Wys≈Çano...");
    }
});
</script>
</body>
</html>
'''

def write_file(path, content):
    full_path = os.path.join(*path.split("/"))
    os.makedirs(os.path.dirname(full_path), exist_ok=True)
    with open(full_path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"‚úÖ Zapisano: {full_path}")

if __name__ == "__main__":
    print("=== START AKTUALIZACJI REGISLITE DO WERSJI 6.0 ===")
    write_file("src/ai/chatgpt_client.py", ai_client_code)
    write_file("src/rtc/signaling.py", signaling_code)
    write_file("src/debugger/debugger_loop.py", debugger_loop_code)
    write_file("src/static/dashboard.html", dashboard_code)
    print("\nüöÄ WSZYSTKO GOTOWE! Zrestartuj serwer (start.bat) i ciesz siƒô nowymi mocami!")